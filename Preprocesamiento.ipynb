{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gpstk\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', np.RankWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Get stations baseline < 100 Km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"conus_2003_324_324.csv\")\n",
    "data.columns=[\"Station\", \"a\",\"b\",\"c\",\"x\",\"y\",\"z\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=data.drop([\"a\",\"b\",\"c\"],axis=1)\n",
    "array=data.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_pairs(array):\n",
    "    distances=[]\n",
    "    for i in range(len(array)):\n",
    "        for j in range(i+1,len(array)):\n",
    "            distance=np.sqrt((array[i][1]-array[j][1])**2+(array[i][2]-array[j][2])**2+(array[i][3]-array[j][3])**2)\n",
    "            if distance<100e3:\n",
    "                distances.append([array[i][0],array[j][0]])\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Stations with baseline less than 100 Km  455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['1ulm', 'sihs'],\n",
       " ['1ulm', 'wnfl'],\n",
       " ['abq1', 'nmsf'],\n",
       " ['abq1', 'zab1'],\n",
       " ['acu1', 'npri'],\n",
       " ['adks', 'ang1'],\n",
       " ['adks', 'ang2'],\n",
       " ['adks', 'lkhu'],\n",
       " ['adks', 'netp'],\n",
       " ['adks', 'txhu']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs=get_pairs(array)\n",
    "print \"Number of Stations with baseline less than 100 Km \",len(pairs)\n",
    "pairs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adjust_times(df):\n",
    "    df=df.sort(\"TIME\")\n",
    "    times=df.TIME.as_matrix()\n",
    "    for i in range(times.size):\n",
    "        if times[i]%30!=0:\n",
    "            times[i]=times[i]+(30-times[i]%30)\n",
    "    df[\"TIME\"]=times\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def label_arcs(df):\n",
    "    time=df.TIME.as_matrix()\n",
    "    diff=np.diff(time)>3600\n",
    "    diff=np.hstack((np.array([False]),diff))\n",
    "    split=np.where(diff==True) #points where there is a \"True\" value\n",
    "    \n",
    "    if len(split[0])>0:\n",
    "        arcs=np.split(time,split[0])\n",
    "        arcsID=[]\n",
    "        n=1\n",
    "        for i in range(len(arcs)):\n",
    "            size=len(arcs[i])\n",
    "            tmp=np.empty(size)\n",
    "            tmp[:]=n\n",
    "            arcsID.append(tmp)\n",
    "            n+=1\n",
    "        arcsID=np.concatenate(arcsID)\n",
    "    else:#one arc\n",
    "        arcsID=np.empty(time.size)\n",
    "        arcsID[:]=1\n",
    "        \n",
    "    df[\"ARCS\"]=arcsID\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cycle_slips(PhaseDelay,L1,L2,threshold=0.5):\n",
    "    slips=[]\n",
    "    slips=np.where(np.abs(np.diff(np.hstack(([0],PhaseDelay))))>threshold) \n",
    "    #noL1=np.where(L1==np.nan)[0] #is nan?\n",
    "    #noL2=np.where(L2==np.nan)[0]\n",
    "    #print \"Slips\",slips\n",
    "    noL1=np.where(np.isnan(L1)==True)[0]#is nan?\n",
    "    noL2=np.where(np.isnan(L2)==True)[0]\n",
    "    if len(noL1)>0:\n",
    "        print \"No L1\", noL1\n",
    "    if len(noL2)>0:\n",
    "        print \"No L2\", noL2\n",
    "    \n",
    "    return slips[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def subarcs(arc,dfarc,splits):\n",
    "    time=dfarc.TIME\n",
    "    if len(splits)>0:\n",
    "        subarcs=np.split(time,splits)\n",
    "        subarcsID=[]\n",
    "        n=arc*10\n",
    "        for i in range(len(subarcs)):\n",
    "            size=len(subarcs[i])\n",
    "            tmp=np.empty(size)\n",
    "            tmp[:]=n\n",
    "            subarcsID.append(tmp)\n",
    "            n+=1\n",
    "        subarcsID=np.concatenate(subarcsID)\n",
    "    else:#one arc\n",
    "        subarcsID=np.empty(time.size)\n",
    "        subarcsID[:]=arc*10+1\n",
    "    dfarc[\"SUBARCS\"]=subarcsID\n",
    "    return dfarc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def del_arcs(df): #delete arcs with less than 10 point\n",
    "    subarcs=df.SUBARCS.values\n",
    "    times=[] #Delete this in the other station\n",
    "    for subarc in np.unique(subarcs):\n",
    "        if len(np.where(df.SUBARCS==subarc)[0])<10:#.SUBARCS\n",
    "            #print \"Subarc id\",subarc,\" deleted with \",len(np.where(df.SUBARCS==subarc)[0]),\" datapoints\"\n",
    "            del_points =np.where(df.SUBARCS==subarc)[0] #Save times so we can delete in other station\n",
    "            times.append(df.TIME[del_points].values)\n",
    "            df=df[df.SUBARCS!=subarc]\n",
    "            \n",
    "    if len(times)>0:\n",
    "        times=np.concatenate(times)\n",
    "        times=times[np.isnan(times)==False]\n",
    "        times=np.unique(times)\n",
    "    else:\n",
    "        times=None\n",
    "    \n",
    "    return df,times\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def poly_fit(dfarc):\n",
    "    #receives a dataframe with a number of subarcs\n",
    "    #On each subarc takes N elements from LI=L1-L2 and performs interpolation, \n",
    "    \n",
    "    f1,f2=gpstk.L1_FREQ_GPS,gpstk.L2_FREQ_GPS\n",
    "    alfa=1.0/((f1**2/f2**2)-1)\n",
    "    #lists potential outliers and time when it occurs on each subarc\n",
    "    times,gradients=[],[]\n",
    "    subarcs=np.unique(dfarc.SUBARCS.values)\n",
    "    for subarc in subarcs:\n",
    "        dfsubarc=dfarc[dfarc.SUBARCS==subarc]\n",
    "        N=10 #window \n",
    "        tPoly ,Poly=[],[]\n",
    "        lI=alfa*(dfsubarc.L1.as_matrix()-dfsubarc.L2.as_matrix())\n",
    "        time=dfsubarc.TIME.as_matrix()\n",
    "\n",
    "        for i in range(0,lI.size,N): \n",
    "            x=np.array(time[i:i+N])\n",
    "            y=np.array(lI[i:i+N])\n",
    "            z= np.polyfit(x,y,2)\n",
    "            p = np.poly1d(z)\n",
    "            for i in range(x.size):\n",
    "                Poly.append(p(x[i]))\n",
    "                tPoly.append(x[i]) \n",
    "                \n",
    "        Poly=np.array(Poly)\n",
    "        \n",
    "        #detects datajumps in the diference between the polinomyal fit and real data \n",
    "        residual=lI-Poly\n",
    "        candidates=np.where(np.abs(np.diff(np.hstack(([0],residual))))>0.8)[0]\n",
    "\n",
    "        if len(candidates)>0:\n",
    "            time_candidates=time[candidates]\n",
    "            gradient_candidates=residual[candidates]\n",
    "            times.append(time_candidates) #array candidatos en subarcos\n",
    "            gradients.append(gradient_candidates)\n",
    "        else:\n",
    "            times.append(np.array([]))\n",
    "            gradients.append(np.array([]))\n",
    "    \n",
    "        dfarc.POLYFIT[dfarc.SUBARCS==subarc]=Poly\n",
    "\n",
    "    return dfarc,times,gradients #df con columna de interpolacion,  array de arrays(algunos vacios), array dde arrays o None (uno por cada sub arco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def outlier_detect(dfarc,k=10):#k=30? 15 min\n",
    "    f1,f2=gpstk.L1_FREQ_GPS,gpstk.L2_FREQ_GPS\n",
    "    alfa=1.0/((f1**2/f2**2)-1)\n",
    "    #lists outlier factors on each arc\n",
    "    outliers_subarcs,times_subarcs=[],[]\n",
    "    subarcs=np.unique(dfarc.SUBARCS.values)\n",
    "    for subarc in subarcs:\n",
    "        dfsubarc=dfarc[dfarc.SUBARCS==subarc]\n",
    "        lI=alfa*(dfsubarc.L1.as_matrix()-dfsubarc.L2.as_matrix())\n",
    "        times=dfsubarc.TIME.as_matrix()\n",
    "        times_subarcs.append(times)\n",
    "        outliers=[] #set of outlier factors for every element in L=L1-L2\n",
    "    \n",
    "        for i in range(0,lI.size):\n",
    "            if i<(k/2+1):\n",
    "                neighbours=np.hstack((lI[0:i],lI[i+1:i+(k/2)+1])) #neighbours around i, without i\n",
    "                tn=np.hstack((times[0:i],times[i+1:i+(k/2)+1]))\n",
    "\n",
    "            elif i>lI.size-(k/2+1):\n",
    "                neighbours=np.hstack((lI[i-k/2:i],lI[i+1:lI.size+1]))\n",
    "                tn=np.hstack((times[i-k/2:i],times[i+1:lI.size+1])) #times neighbour\n",
    "\n",
    "            else:\n",
    "                neighbours=np.hstack((lI[i-k/2:i],lI[i+1:i+(k/2)+1]))\n",
    "                tn=np.hstack((times[i-k/2:i],times[i+1:i+(k/2)+1]))\n",
    "\n",
    "            OFt=0\n",
    "            deno=np.sum(1/(np.abs(times[i]-tn)*1.0))#denominator de Wpq para elemento i\n",
    "            for neighbour in range(neighbours.size): \n",
    "                if times[neighbour]!=times[i]:\n",
    "                    Wpq=1/np.abs(times[i]-times[neighbour])\n",
    "                    Wpq=Wpq/deno\n",
    "                    OFt+=(Wpq*np.abs(lI[i]-lI[neighbour]))\n",
    "            outliers.append(OFt)  #outliers in the subarc\n",
    "        outliers_subarcs.append(outliers) #outliers on arc \n",
    "\n",
    "    return outliers_subarcs,times_subarcs #list of lists(suarcs),list of arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_outliers(ptimes,otimes,poly_outliers,outliers_factor,subarcs):\n",
    "    confirmed=[] #times of confirmed outliers\n",
    "    print \"# Subarcs: \",subarcs\n",
    "    print \"Polinomial\",poly_outliers\n",
    "    for subarc in range(subarcs):\n",
    "        if len(poly_outliers[subarc])>0:\n",
    "            for i in range(len(poly_outliers[subarc])):\n",
    "                print \"Quedan\", len(poly_outliers[subarc]),\" candidatos\" \n",
    "                #check for max outliers with polinomial method\n",
    "                maxpoly=np.argmax(poly_outliers[subarc])\n",
    "                valuemaxpoly=poly_outliers[subarc][maxpoly] \n",
    "                tmaxpoly=ptimes[subarc][maxpoly]\n",
    "                #check for max outliers with outliers factor method\n",
    "                maxofactor=np.argmax(outliers_factor[subarc])\n",
    "                valuemaxout=outliers_factor[subarc][maxofactor]\n",
    "                tmaxofactor=otimes[subarc][maxofactor]\n",
    "\n",
    "                if tmaxofactor==tmaxpoly:\n",
    "                    if tmaxofactor not in confirmed:\n",
    "                        confirmed.append(tmaxofactor)\n",
    "                        print \"Outlier detectado en t=\",tmaxofactor\n",
    "                        #Save time of confirmed outlier\n",
    "                        print \"Verificar que elemento se elimina\"\n",
    "                        print len(outliers_factor[subarc])\n",
    "                        print len(otimes[subarc])\n",
    "                        print poly_outliers[subarc].shape\n",
    "                        print ptimes[subarc].shape\n",
    "                        #Delete point from both methods and continue \n",
    "                        outliers_factor[subarc]=np.delete(outliers_factor[subarc],maxofactor)\n",
    "                        otimes[subarc]=np.delete(otimes[subarc],maxofactor)\n",
    "                        poly_outliers[subarc]=np.delete(poly_outliers[subarc],maxpoly)\n",
    "                        ptimes[subarc]=np.delete(ptimes[subarc],maxpoly)\n",
    "                        print len(outliers_factor[subarc])\n",
    "                        print len(otimes[subarc])\n",
    "                        print poly_outliers[subarc].shape\n",
    "                        print ptimes[subarc].shape\n",
    "                    else:\n",
    "                        print \"Ya estaba confirmado\"\n",
    "                        break\n",
    "                        \n",
    "\n",
    "                else: #If there aret any more coincidences stop searching\n",
    "                    print \"No hay mas confirmados\"\n",
    "                    break\n",
    "\n",
    "        else:\n",
    "            #See next subarc\n",
    "            print \"No hay candidatos con metodo polinomial fit\"\n",
    "            continue\n",
    "            \n",
    "    return confirmed\n",
    "            \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of ten Pairs of stations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Subarcs:  1\n",
      "Polinomial [array([], dtype=float64)]\n",
      "No hay candidatos con metodo polinomial fit\n",
      "# Subarcs:  1\n",
      "Polinomial [array([ 0.87293814, -0.92452953,  0.83139617, -0.71471635, -1.46130593,\n",
      "        4.3490522 , -1.87615798, -0.30972705,  0.79456928, -0.26971112,\n",
      "        1.62879513, -1.79988355,  0.32942796, -1.18830028,  3.06371093,\n",
      "       -1.53096966,  1.44815091, -1.26991439,  3.4038376 , -1.86580963,\n",
      "       -0.3062582 ,  0.79449206, -1.78302233, -0.12158365,  0.95128535,\n",
      "       -0.05409582,  0.8808661 , -1.65945032,  2.55146826, -3.19703094,\n",
      "       -0.03408492,  1.09657683,  2.74472196, -2.91040302,  0.58143204,\n",
      "        1.09977536, -0.697014  ,  0.94505329, -0.70215084,  0.10707826,\n",
      "        1.21736232, -0.87333042, -1.33206905, -0.29267605,  1.31646273,\n",
      "       -0.98998202,  0.95388479, -0.18501476,  1.07559595, -0.81453449,\n",
      "       -2.92294405,  3.99524147, -0.28358465,  0.96312851, -0.57989579,\n",
      "        1.22334021, -0.90285772, -0.04815897,  1.47853331, -1.37061275,\n",
      "        1.12236053, -0.88426351,  1.19786446, -0.40413713,  1.30266513,\n",
      "        0.2815396 , -0.87079924,  0.71178418, -1.38104521, -0.05024212,\n",
      "        1.03241193,  0.68017105, -0.67447844,  0.46794939, -0.36058253,\n",
      "        0.66628861, -1.00711698,  0.55541398, -0.27909699, -1.15756726,\n",
      "        1.8686218 , -0.98800085,  2.17630716,  0.52310362, -1.77120961,\n",
      "        0.059205  ,  1.11836822, -1.03099453,  0.59938577, -1.7959599 ,\n",
      "        0.83977394,  0.41864481,  1.67564961, -0.2703409 , -1.95794032,\n",
      "       -0.07197986,  1.464284  , -0.63101845, -0.35743687,  0.47371951,\n",
      "       -1.7408371 ,  1.07957959, -1.47045472,  0.40840592,  0.52410766,\n",
      "       -0.92554773,  0.29206005, -1.66950323,  1.12481586, -0.64513901,\n",
      "        1.62979063, -0.68315202,  0.74615735, -0.84252752,  1.54124853,\n",
      "       -0.8380405 ,  1.01001269, -1.23431937,  0.65752923,  0.59166006,\n",
      "       -0.68613792,  0.81796914, -0.57483681, -0.84087386,  0.05296829,\n",
      "       -0.8547826 ,  0.50125252,  0.96591347, -0.89387753, -0.01584751,\n",
      "       -0.84235495,  0.24348893, -0.18513546,  0.49712207, -0.6631799 ,\n",
      "        0.21811822, -0.34244366, -0.75358233,  0.24380819,  0.46059418,\n",
      "       -0.32025073,  0.55051745, -0.63245849, -0.6569188 , -0.44014995,\n",
      "        0.85489152, -0.07236819, -1.41251977,  1.20906302,  0.29929338,\n",
      "       -0.84109643,  0.30798692, -0.66589327,  0.16985319, -0.43062914,\n",
      "        0.57776324,  0.56705114, -1.1188954 ,  0.51497409,  0.8143653 ,\n",
      "       -1.15242081,  0.12485354,  0.5567017 , -1.1563212 ,  0.73115157,\n",
      "       -0.29750413,  1.02957384, -0.18542401, -1.56594347,  0.31000485,\n",
      "       -1.00324323,  0.45634499,  0.13105459, -1.11945964,  1.35878143,\n",
      "       -0.59560517,  0.57509533, -1.39449068, -0.18696954,  0.75312055,\n",
      "       -0.16931874, -0.95979314,  0.55630912, -0.75092775,  1.28417931,\n",
      "        0.07079238, -0.43620165,  0.68837303, -0.64374322,  1.03307566,\n",
      "       -0.22484707,  0.74269646, -1.31553292,  1.22992981, -0.35574576,\n",
      "       -1.04877574,  0.98696545, -0.70973524,  0.17201664, -1.64318522,\n",
      "        0.9863698 ,  0.08902343, -0.55262504,  0.85484193, -1.69761591,\n",
      "        0.62018777, -0.20889603,  0.8216368 , -1.13443398,  0.23443688,\n",
      "        1.12967171, -1.73522573, -0.53245644, -1.64627232,  2.46112533,\n",
      "       -2.77210709,  0.3663251 , -0.82573818,  1.44676019, -0.56890173,\n",
      "        0.52469253,  0.98930609, -1.06996044,  0.49555568, -0.44216239,\n",
      "       -1.44978335,  1.57312255, -0.77899726,  2.17697415, -0.03948043,\n",
      "       -0.87350259,  1.29740916, -0.73024693,  0.96668602, -0.91300622,\n",
      "        0.86402996, -0.10492533, -0.97840734, -1.4188109 ,  1.70985019,\n",
      "        0.4599762 , -1.15737268, -0.73407415,  3.00113679,  0.11243849,\n",
      "       -0.75155936,  0.27367003, -0.99502679,  0.50908688, -2.08605173,\n",
      "        0.98873937,  2.85293736, -2.73474482,  1.0363001 , -0.34122609,\n",
      "        2.5277141 , -3.02372457, -0.65946139,  1.10211068])]\n",
      "Quedan 259  candidatos\n",
      "Outlier detectado en t= 47910.0\n",
      "Verificar que elemento se elimina\n",
      "777\n",
      "777\n",
      "(259,)\n",
      "(259,)\n",
      "776\n",
      "776\n",
      "(258,)\n",
      "(258,)\n",
      "Quedan 258  candidatos\n",
      "No hay mas confirmados\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "dir_txt=\"txtcors/\"\n",
    "files = [f for f in listdir(dir_txt)]\n",
    "\n",
    "f1=gpstk.L1_FREQ_GPS\n",
    "f2=gpstk.L2_FREQ_GPS\n",
    "factor_alfa=f2**2/(f1**2-f2**2)\n",
    "c=3e8\n",
    "alfa=1.0/((f1**2/f2**2)-1) \n",
    "\n",
    "for stations in pairs[:1]:\n",
    "    #Load Files\n",
    "    st1,st2=stations[0],stations[1]\n",
    "    columns=[\"PRN\",\"TIME\",\"C1\",\"C2\",\"L1\",\"L2\",\"Tgd\",\"IPP\",\"Elevation\",\"Azimuth\"]\n",
    "    file1=dir_txt+[f for f in files if st1 in f ][0]\n",
    "    file2=dir_txt+[f for f in files if st2 in f ][0]\n",
    "    \n",
    "    df1=pd.read_csv(file1,sep=\",\")\n",
    "    df1.columns=columns\n",
    "    df2=pd.read_csv(file2,sep=\",\")\n",
    "    df2.columns=columns\n",
    "    df1,df2=adjust_times(df1),adjust_times(df2)\n",
    "    df3=pd.merge(df1,df2,on=[\"TIME\",\"PRN\",\"Tgd\"])\n",
    "    \n",
    "    #For each satellite observed by the stations\n",
    "    for sat in np.unique(df3.PRN.values):\n",
    "    ##\n",
    "        df=df3[df3.PRN==sat] #dataframe with times of an specific satellite\n",
    "        df=df.reset_index(drop=True)\n",
    "        #Estimate delay measures \n",
    "        df[\"PhaseDelay_x\"]=alfa*(df.L1_x-df.L2_x)\n",
    "        df[\"PhaseDelay_y\"]=alfa*(df.L1_y-df.L2_y)\n",
    "        df[\"CodeDelay_x\"]=alfa*(df.C2_x-df.C1_x)\n",
    "        df[\"CodeDelay_y\"]=alfa*(df.C2_y-df.C1_y)\n",
    "        #Add column with indicators of time separation\n",
    "        df=label_arcs(df)\n",
    "       \n",
    "        for arc in np.unique(df.ARCS.values):\n",
    "            #Search for cycle Slips on each arc\n",
    "            dfarc=df[df.ARCS==arc]\n",
    "            slips1=cycle_slips(dfarc.PhaseDelay_x.as_matrix(),dfarc.L1_x.as_matrix(),dfarc.L2_x.as_matrix(),2.5)\n",
    "            slips2=cycle_slips(dfarc.PhaseDelay_y.as_matrix(),dfarc.L1_y.as_matrix(),dfarc.L2_y.as_matrix(),2.5)\n",
    "            #Dataframes with subarcs on stations\n",
    "            dfarc1=subarcs(arc,dfarc,slips1)\n",
    "            dfarc1=dfarc1.drop([\"PhaseDelay_y\",\"CodeDelay_y\",\"L1_y\",\"L2_y\",\"C1_y\",\"C2_y\",\"IPP_y\",\"Elevation_y\",\"Azimuth_y\"],axis=1)\n",
    "            dfarc2=subarcs(arc,dfarc,slips2)\n",
    "            dfarc2=dfarc2.drop([\"PhaseDelay_x\",\"CodeDelay_x\",\"L1_x\",\"L2_x\",\"C1_x\",\"C2_x\",\"IPP_x\",\"Elevation_x\",\"Azimuth_x\"],axis=1)\n",
    "            #Remove short-arcs \n",
    "            new_dfarc1,times1=del_arcs(dfarc1)\n",
    "            new_dfarc2,times2=del_arcs(dfarc2)\n",
    "            if times1!=None:\n",
    "                for t in times1:\n",
    "                    new_dfarc2=new_dfarc2[new_dfarc2.TIME!=t]\n",
    "            if times2!=None:\n",
    "                for t in times2:\n",
    "                    new_dfarc1=new_dfarc1[new_dfarc1.TIME!=t]\n",
    "            \n",
    "            columns=['PRN','TIME','C1','C2','L1','L2','Tgd', 'IPP', 'Elevation', 'Azimuth', 'PhaseDelay', 'CodeDelay', 'ARCS', 'SUBARCS']\n",
    "            new_dfarc1.columns=columns\n",
    "            new_dfarc2.columns=columns\n",
    "            #Polinomial fit and outlier detection\n",
    "            new_dfarc1[\"POLYFIT\"]=np.nan\n",
    "            new_dfarc2[\"POLYFIT\"]=np.nan\n",
    "            new_dfarc1,ptimes1,poly_outliers1=poly_fit(new_dfarc1)\n",
    "            new_dfarc2,ptimes2,poly_outliers2=poly_fit(new_dfarc2)\n",
    "            #Outliers factor\n",
    "            outliers1,otimes1=outlier_detect(new_dfarc1,10)\n",
    "            outliers2,otimes2=outlier_detect(new_dfarc2,10)\n",
    "            #if len(ptimes1) != len(otimes1):\n",
    "                #print len(ptimes1), len(otimes1)\n",
    "            #Confirm and delete outliers\n",
    "            #station 1\n",
    "            subarcs1=len(np.unique(new_dfarc1.SUBARCS.values))\n",
    "            times1=remove_outliers(ptimes1,otimes1,poly_outliers1,outliers1,subarcs1)\n",
    "            #station 2\n",
    "            subarcs2=len(np.unique(new_dfarc2.SUBARCS.values))\n",
    "            times2=remove_outliers(ptimes2,otimes2,poly_outliers2,outliers2,subarcs2)\n",
    "            #Both dataframes now should have same number of observations and we can merge them\n",
    "            \n",
    "            #print np.unique(dfarc2.SUBARCS.values)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"*******************************************************************************\"\n",
    "print \"****************************POLYNOMIAL FIT*************************************\"\n",
    "print \"Candidates Times st1: \",ptimes1,\"\\n\"\n",
    "print \"Outliers: \",poly_outliers1\n",
    "print \"Candidates Times st2: \",ptimes2,\"\\n\"\n",
    "print \"Outliers: \",poly_outliers2\n",
    "#Outliers removal\n",
    "print \"****************************OUTLIER FACTOR METHOD*************************************\"\n",
    "print \"Candidates Times st1: \",otimes1,\"\\n\"\n",
    "print \"Outliers: \",outliers1\n",
    "print \"Candidates Times st2: \",otimes2,\"\\n\"\n",
    "print \"Outliers: \",outliers2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Subarcs\",np.unique(new_dfarc1.SUBARCS.values)\n",
    "print \"Subarcs\",np.unique(new_dfarc2.SUBARCS.values)\n",
    "print \n",
    "#new_dfarc1.PhaseDelay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print len(ptimes1[0])\n",
    "print len(otimes1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(np.unique(new_dfarc1.SUBARCS.values))\n",
    "np.unique(new_dfarc1.SUBARCS.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subarc=0\n",
    "print len(poly_outliers1[subarc])>0\n",
    "print poly_outliers1\n",
    "print outliers1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(ptimes2) #un solo subarco\n",
    "print ptimes2[0].size\n",
    "print poly_outliers2[0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(otimes1[0]) #un solo sub arco\n",
    "print len(outliers1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(new_dfarc1.TIME[new_dfarc1.SUBARCS==17.],new_dfarc1.PhaseDelay[new_dfarc1.SUBARCS==17.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(new_dfarc1.TIME[new_dfarc1.SUBARCS==17.],new_dfarc1.POLYFIT[new_dfarc1.SUBARCS==17.])\n",
    "print poly_outliers1\n",
    "print poly_outliers2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_dfarc1[new_dfarc1.SUBARC==17.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_dfarc2.TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plt.scatter(dfarc1.TIME,dfarc1.SUBARCS)\n",
    "#print np.unique(dfarc1.SUBARCS.values)\n",
    "#dfarc1[dfarc1.SUBARCS==11].SUBARCS\n",
    "#Compare before and after remove short arcs\n",
    "#plt.ylim(-1000,100000)\n",
    "print np.unique(dfarc1.SUBARCS.values)\n",
    "plt.scatter(dfarc1.TIME,dfarc1.PhaseDelay_x,color=\"r\",alpha=.5)\n",
    "#for subarc in \n",
    "#plt.scatter(new_dfarc.TIME,new_dfarc1.PhaseDelay_x,color=\"b\",alpha=.5)\n",
    "#again merge by time, this removes short arcs y both stations\n",
    "#print np.unique(new_dfarc1.SUBARCS.values)\n",
    "#print np.unique(new_dfarc1.SUBARCS.values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
