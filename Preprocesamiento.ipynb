{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gpstk\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', np.RankWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Get stations baseline < 100 Km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"conus_2003_324_324.csv\")\n",
    "data.columns=[\"Station\", \"a\",\"b\",\"c\",\"x\",\"y\",\"z\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=data.drop([\"a\",\"b\",\"c\"],axis=1)\n",
    "array=data.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_pairs(array):\n",
    "    distances=[]\n",
    "    for i in range(len(array)):\n",
    "        for j in range(i+1,len(array)):\n",
    "            distance=np.sqrt((array[i][1]-array[j][1])**2+(array[i][2]-array[j][2])**2+(array[i][3]-array[j][3])**2)\n",
    "            if distance<100e3:\n",
    "                distances.append([array[i][0],array[j][0]])\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Stations with baseline less than 100 Km  455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['1ulm', 'sihs'],\n",
       " ['1ulm', 'wnfl'],\n",
       " ['abq1', 'nmsf'],\n",
       " ['abq1', 'zab1'],\n",
       " ['acu1', 'npri'],\n",
       " ['adks', 'ang1'],\n",
       " ['adks', 'ang2'],\n",
       " ['adks', 'lkhu'],\n",
       " ['adks', 'netp'],\n",
       " ['adks', 'txhu']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs=get_pairs(array)\n",
    "print \"Number of Stations with baseline less than 100 Km \",len(pairs)\n",
    "pairs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adjust_times(df):\n",
    "    df=df.sort(\"TIME\")\n",
    "    times=df.TIME.as_matrix()\n",
    "    for i in range(times.size):\n",
    "        if times[i]%30!=0:\n",
    "            times[i]=times[i]+(30-times[i]%30)\n",
    "    df[\"TIME\"]=times\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def label_arcs(df):\n",
    "    time=df.TIME.as_matrix()\n",
    "    diff=np.diff(time)>3600\n",
    "    diff=np.hstack((np.array([False]),diff))\n",
    "    split=np.where(diff==True) #points where there is a \"True\" value\n",
    "    \n",
    "    if len(split[0])>0:\n",
    "        arcs=np.split(time,split[0])\n",
    "        arcsID=[]\n",
    "        n=1\n",
    "        for i in range(len(arcs)):\n",
    "            size=len(arcs[i])\n",
    "            tmp=np.empty(size)\n",
    "            tmp[:]=n\n",
    "            arcsID.append(tmp)\n",
    "            n+=1\n",
    "        arcsID=np.concatenate(arcsID)\n",
    "    else:#one arc\n",
    "        arcsID=np.empty(time.size)\n",
    "        arcsID[:]=1\n",
    "        \n",
    "    df[\"ARCS\"]=arcsID\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cycle_slips(PhaseDelay,L1,L2,threshold=0.5):\n",
    "    slips=[]\n",
    "    slips=np.where(np.abs(np.diff(np.hstack(([0],PhaseDelay))))>threshold) \n",
    "    #noL1=np.where(L1==np.nan)[0] #is nan?\n",
    "    #noL2=np.where(L2==np.nan)[0]\n",
    "    #print \"Slips\",slips\n",
    "    noL1=np.where(np.isnan(L1)==True)[0]#is nan?\n",
    "    noL2=np.where(np.isnan(L2)==True)[0]\n",
    "    if len(noL1)>0:\n",
    "        print \"No L1\", noL1\n",
    "    if len(noL2)>0:\n",
    "        print \"No L2\", noL2\n",
    "    \n",
    "    return slips[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def subarcs(arc,dfarc,splits):\n",
    "    time=dfarc.TIME\n",
    "    if len(splits)>0:\n",
    "        subarcs=np.split(time,splits)\n",
    "        subarcsID=[]\n",
    "        n=arc*10\n",
    "        for i in range(len(subarcs)):\n",
    "            size=len(subarcs[i])\n",
    "            tmp=np.empty(size)\n",
    "            tmp[:]=n\n",
    "            subarcsID.append(tmp)\n",
    "            n+=1\n",
    "        subarcsID=np.concatenate(subarcsID)\n",
    "    else:#one arc\n",
    "        subarcsID=np.empty(time.size)\n",
    "        subarcsID[:]=arc*10+1\n",
    "    dfarc[\"SUBARCS\"]=subarcsID\n",
    "    return dfarc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def del_arcs(df): #delete arcs with less than 10 point\n",
    "    subarcs=df.SUBARCS.values\n",
    "    times=[] #Delete this in the other station\n",
    "    for subarc in np.unique(subarcs):\n",
    "        if len(np.where(df.SUBARCS==subarc)[0])<10:#.SUBARCS\n",
    "            #print \"Subarc id\",subarc,\" deleted with \",len(np.where(df.SUBARCS==subarc)[0]),\" datapoints\"\n",
    "            del_points =np.where(df.SUBARCS==subarc)[0] #Save times so we can delete in other station\n",
    "            times.append(df.TIME[del_points].values)\n",
    "            df=df[df.SUBARCS!=subarc]\n",
    "            \n",
    "    if len(times)>0:\n",
    "        times=np.concatenate(times)\n",
    "        times=times[np.isnan(times)==False]\n",
    "        times=np.unique(times)\n",
    "    else:\n",
    "        times=None\n",
    "    \n",
    "    return df,times\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def poly_fit(dfarc):\n",
    "    #receives a dataframe with a number of subarcs\n",
    "    #On each subarc \n",
    "    #takes N elements from LI=L1-L2 and performs interpolation, \n",
    "    #detects datajumps in the diference between the polinomyal fit and real data \n",
    "    f1,f2=gpstk.L1_FREQ_GPS,gpstk.L2_FREQ_GPS\n",
    "    alfa=1.0/((f1**2/f2**2)-1)\n",
    "    subarcs=np.unique(dfarc.SUBARCS.values)\n",
    "    for subarc in subarcs:\n",
    "        dfsubarc=dfarc[dfarc.SUBARCS==subarc]\n",
    "        N=10 #window \n",
    "        tPoly ,Poly=[],[]\n",
    "        lI=alfa*(dfsubarc.L1.as_matrix()-dfsubarc.L2.as_matrix())\n",
    "        time=dfsubarc.TIME.as_matrix()\n",
    "\n",
    "        for i in range(0,lI.size,N): \n",
    "            x=np.array(time[i:i+N])\n",
    "            y=np.array(lI[i:i+N])\n",
    "            z= np.polyfit(x,y,2)\n",
    "            p = np.poly1d(z)\n",
    "            for i in range(x.size):\n",
    "                Poly.append(p(x[i]))\n",
    "                tPoly.append(x[i]) \n",
    "                \n",
    "        Poly=np.array(Poly)\n",
    "        residual=lI-Poly\n",
    "        #jumps=np.where(np.abs(np.diff(np.hstack(([0],residual))))>0.8)[0]\n",
    "        \n",
    "        #if jumps.size>0:\n",
    "            #pslip=np.argmax(residual[jumps])\n",
    "            #pslip=jumps[pslip]\n",
    "        #else:\n",
    "            #pslip=None\n",
    "        \n",
    "        dfarc[dfarc.SUBARCS==subarc][\"POLYFIT\"]=Poly\n",
    "    return dfarc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of ten Pairs of stations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "dir_txt=\"txtcors/\"\n",
    "files = [f for f in listdir(dir_txt)]\n",
    "\n",
    "f1=gpstk.L1_FREQ_GPS\n",
    "f2=gpstk.L2_FREQ_GPS\n",
    "factor_alfa=f2**2/(f1**2-f2**2)\n",
    "c=3e8\n",
    "alfa=1.0/((f1**2/f2**2)-1) \n",
    "\n",
    "for stations in pairs[:10]:\n",
    "    #Load Files\n",
    "    st1,st2=stations[0],stations[1]\n",
    "    columns=[\"PRN\",\"TIME\",\"C1\",\"C2\",\"L1\",\"L2\",\"Tgd\",\"IPP\",\"Elevation\",\"Azimuth\"]\n",
    "    file1=dir_txt+[f for f in files if st1 in f ][0]\n",
    "    file2=dir_txt+[f for f in files if st2 in f ][0]\n",
    "    \n",
    "    df1=pd.read_csv(file1,sep=\",\")\n",
    "    df1.columns=columns\n",
    "    df2=pd.read_csv(file2,sep=\",\")\n",
    "    df2.columns=columns\n",
    "    df1,df2=adjust_times(df1),adjust_times(df2)\n",
    "    df3=pd.merge(df1,df2,on=[\"TIME\",\"PRN\",\"Tgd\"])\n",
    "    \n",
    "    #For each satellite observed by the stations\n",
    "    for sat in np.unique(df3.PRN.values):\n",
    "    ##\n",
    "        df=df3[df3.PRN==sat] #dataframe with times of an specific satellite\n",
    "        df=df.reset_index(drop=True)\n",
    "        #Estimate delay measures \n",
    "        df[\"PhaseDelay_x\"]=alfa*(df.L1_x-df.L2_x)\n",
    "        df[\"PhaseDelay_y\"]=alfa*(df.L1_y-df.L2_y)\n",
    "        df[\"CodeDelay_x\"]=alfa*(df.C2_x-df.C1_x)\n",
    "        df[\"CodeDelay_y\"]=alfa*(df.C2_y-df.C1_y)\n",
    "        #Add column with indicators of time separation\n",
    "        df=label_arcs(df)\n",
    "       \n",
    "        for arc in df.ARCS.values:\n",
    "            #Search for cycle Slips on each arc\n",
    "            dfarc=df[df.ARCS==arc]\n",
    "            slips1=cycle_slips(dfarc.PhaseDelay_x.as_matrix(),dfarc.L1_x.as_matrix(),dfarc.L2_x.as_matrix(),2.5)\n",
    "            slips2=cycle_slips(dfarc.PhaseDelay_y.as_matrix(),dfarc.L1_y.as_matrix(),dfarc.L2_y.as_matrix(),2.5)\n",
    "            #Dataframes with subarcs on stations\n",
    "            dfarc1=subarcs(arc,dfarc,slips1)\n",
    "            dfarc1=dfarc1.drop([\"PhaseDelay_y\",\"CodeDelay_y\",\"L1_y\",\"L2_y\",\"C1_y\",\"C2_y\",\"IPP_y\",\"Elevation_y\",\"Azimuth_y\"],axis=1)\n",
    "            dfarc2=subarcs(arc,dfarc,slips2)\n",
    "            dfarc2=dfarc2.drop([\"PhaseDelay_x\",\"CodeDelay_x\",\"L1_x\",\"L2_x\",\"C1_x\",\"C2_x\",\"IPP_x\",\"Elevation_x\",\"Azimuth_x\"],axis=1)\n",
    "            #Remove short-arcs \n",
    "            new_dfarc1,times1=del_arcs(dfarc1)\n",
    "            new_dfarc2,times2=del_arcs(dfarc2)\n",
    "            if times1!=None:\n",
    "                for t in times1:\n",
    "                    new_dfarc2=new_dfarc2[new_dfarc2.TIME!=t]\n",
    "            if times2!=None:\n",
    "                for t in times2:\n",
    "                    new_dfarc1=new_dfarc1[new_dfarc1.TIME!=t]\n",
    "            \n",
    "            columns=['PRN','TIME','C1','C2','L1','L2','Tgd', 'IPP', 'Elevation', 'Azimuth', 'PhaseDelay', 'CodeDelay', 'ARCS', 'SUBARCS']\n",
    "            new_dfarc1.columns=columns\n",
    "            new_dfarc2.columns=columns\n",
    "            #Polinomial fit and outlier detection\n",
    "            new_dfarc1[\"POLYFIT\"]=np.nan\n",
    "            new_dfarc2[\"POLYFIT\"]=np.nan\n",
    "            new_dfarc1=poly_fit(new_dfarc1)\n",
    "            new_dfarc2=poly_fit(new_dfarc2)\n",
    "            #Outliers removal\n",
    "            #Both dataframes now should have same number of observations and we can merge them\n",
    "            \n",
    "            #print np.unique(dfarc2.SUBARCS.values)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.unique(new_dfarc1.SUBARCS.values)[1]\n",
    "new_dfarc1.PhaseDelay_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfsubarc=new_dfarc1[new_dfarc1.SUBARCS==17.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_dfarc1[new_dfarc1.SUBARCS==17.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print times2\n",
    "print new_dfarc1.shape\n",
    "if times2!=None:\n",
    "    for t in times2:\n",
    "        new_dfarc1=new_dfarc1[new_dfarc1.TIME!=t]\n",
    "print new_dfarc1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_dfarc2.TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print times2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plt.scatter(dfarc1.TIME,dfarc1.SUBARCS)\n",
    "#print np.unique(dfarc1.SUBARCS.values)\n",
    "#dfarc1[dfarc1.SUBARCS==11].SUBARCS\n",
    "#Compare before and after remove short arcs\n",
    "#plt.ylim(-1000,100000)\n",
    "print np.unique(dfarc1.SUBARCS.values)\n",
    "plt.scatter(dfarc1.TIME,dfarc1.PhaseDelay_x,color=\"r\",alpha=.5)\n",
    "#for subarc in \n",
    "#plt.scatter(new_dfarc.TIME,new_dfarc1.PhaseDelay_x,color=\"b\",alpha=.5)\n",
    "#again merge by time, this removes short arcs y both stations\n",
    "#print np.unique(new_dfarc1.SUBARCS.values)\n",
    "#print np.unique(new_dfarc1.SUBARCS.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.unique(new_dfarc1.SUBARCS.values)\n",
    "plt.scatter(new_dfarc1.TIME,new_dfarc1.PhaseDelay_x,color=\"r\",alpha=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subarcs=dfarc1.SUBARCS.values\n",
    "print np.unique(subarcs)\n",
    "times=[] #Delete this in the other station\n",
    "for subarc in np.unique(subarcs):\n",
    "    if len(np.where(dfarc1.SUBARCS==subarc)[0])<10:#.SUBARCS\n",
    "        #print \"Subarc id\",subarc,\" deleted with \",len(np.where(df.SUBARCS==subarc)[0]),\" datapoints\"\n",
    "        del_points =np.where(dfarc1.SUBARCS==subarc)[0] #Save times so we can delete in other station\n",
    "        times.append(dfarc1.TIME[del_points].values)\n",
    "        print \"Subarco \",subarc,\"Tiempos a  eliminar\",dfarc1.TIME[del_points].values\n",
    "        dfarc1=dfarc1[dfarc1.SUBARCS!=subarc]\n",
    "\n",
    "if len(times)>0:\n",
    "    times=np.concatenate(times)\n",
    "    times=times[np.isnan(times)==False]\n",
    "    #times=np.unique(times)\n",
    "else:\n",
    "    times=None\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfsubarc[\"POLYFIT\"]=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfsubarc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
